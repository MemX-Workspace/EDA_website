<template>
    <div class="lecture-preview">
        <div class="lecture-details">
            <h3>Lecture I</h3>
            <p><strong>Thursday, July 4, 10:00-12:00</strong></p>
            <h4>Exploring LLMs: Milestones, Techniques, and Challenges</h4>
            <div class="row">
                <div class="speaker-details" style="width: 75%;margin-left: 15px;">
                    <p><strong>Dr. Jiangli Huang</strong></p>
                    <p style="font-size: 0.8; margin-bottom: 0;">Postdoc at the School of
                        Computer Science, Fudan University.</p>
                    <p style="font-size: 0.8rem; margin-bottom: 0;">Publications in leading
                        EDA conferences and journals including DAC,
                        TCASI, TMTT.</p>
                    <p style="font-size: 0.8rem; margin-bottom: 0;">Pioneer of introducing
                        LLMs into analog circuit design.</p>
                </div>
                <img src="img/photos/Jiangli_Huang.png" alt="Dr. Jiangli Huang" class="speaker-photo"
                    style="width: 100px; height: 100px;object-fit: cover;">
            </div>
            <div class="lecture-description">
                <p style="font-size: 1.0rem;margin-top: 10px;">
                    LLMs have captured significant attention owing to their impressive
                    capabilities across a diverse array
                    of natural language processing tasks. These models achieve
                    general-purpose language understanding and
                    generation by leveraging billions of parameters trained on extensive
                    text data. The research area of LLMs,
                    while very recent, is evolving rapidly in many different ways. In this
                    lecture, we will delve into the
                    evolution of LLMs and spotlight the most influential models, including
                    prominent families such as GPT,
                    LLaMA, and PaLM. We will examine their unique characteristics, pivotal
                    contributions, and inherent
                    limitations. Further, we will provide an overview of innovative
                    techniques developed for constructing and
                    enhancing LLMs. Concluding our presentation, we will discuss the ongoing
                    challenges in the field and
                    speculate on potential research directions for the future of LLMs.
                </p>
            </div>
        </div>
    </div>
</template>
